# -*- coding: utf-8 -*-
"""Stress_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uIc_5VpT9vvf44WvIRIypEAf3RAahab2
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
import pandas as pd

!ls "/content/drive/My Drive/BDA_Project/BDA_Project"

df = pd.read_csv('/content/drive/My Drive/BDA_Project/BDA_Project/new_dataset.csv')
df.head()

df.isnull().any()

df.isnull().sum()

df.info()

print(df.shape)

df = df.dropna(axis=0, how='any')

df.isnull().any()

df.shape

df.colums = ['PP', 'Condition', 'HR', 'RMSSD', 'SCL', 'Squaltiy', 'Sneutral', 'Shappy', 'Ssad', 'Sangry', 'Ssurprised', 'Sscared', 'Sdisgusted', 'Svalence', 'SyHeadOrientation','SxHeadOrientation', 'SzHeadOrientation', 'SmouthOpen', 'SleftEyeClosed', 'SrightEyeClosed','SleftEyebrowLowered', 'SleftEyebroRaised', 'SrightEyebrowLowered', 'SrightEyebroRaised', 'SgazeDirectionForward','SgazeDirectionLeft','SgazeDirectionRight']

df.rename(columns={'Unnamed: 5':'Squaltiy', 'Unnamed: 6':'Sneutral', 'Unnamed: 7':'Shappy', 'Unnamed: 8':'Ssad', 'Unnamed: 9':'Sangry', 'Unnamed: 10':'Ssurprised', 'Unnamed: 11':'Sscared', 'Unnamed: 12':'Sdisgusted', 'Unnamed: 13':'Svalence', 'Unnamed: 14':'SyHeadOrientation','Unnamed: 15':'SxHeadOrientation', 'Unnamed: 16':'SzHeadOrientation', 'Unnamed: 17':'SmouthOpen', 'Unnamed: 18':'SleftEyeClosed', 'Unnamed: 19':'SrightEyeClosed','Unnamed: 20':'SleftEyebrowLowered', 'Unnamed: 21':'SleftEyebrowRaised', 'Unnamed: 22':'SrightEyebrowLowered', 'Unnamed: 23':'SrightEyebrowRaised', 'Unnamed: 24':'SgazeDirectionForward','Unnamed: 25':'SgazeDirectionLeft','Unnamed: 26':'SgazeDirectionRight'}, inplace=True)

df['Condition'].unique()

df.info()

df['pp'] = pd.to_numeric(df['pp'], errors='coerce')
print(df.dtypes)

df

df.boxplot(column=['RMSSD'])

grouped = df.groupby(['Condition'])
grouped.aggregate(np.sum)
grouped.size()

sns.countplot(df["Condition"])
print(df.Condition.value_counts())

p=df['HR'].max()
q=df['HR'].min()
print("Minimum::", q)
print("Maximum::", p)
r=p-q
print("Range::",r)
n=3
a=df['HR'].value_counts(bins=n)
print("\n\n-------------Stress or Non Stress HR Data-------------")
print(a)
df_filtered_1 = df.query('Condition == 1') #stress
df_filtered_0 = df.query('Condition == 0') #non stress

a1 = df_filtered_1['HR'].value_counts(bins=n) #stress
a0 = df_filtered_0['HR'].value_counts(bins=n) #non stress

print("\n------------------Stress Data HR----------------")
print(a1)
a1.plot()

p=df['RMSSD'].max()
q=df['RMSSD'].min()
print("Minimum::", q)
print("Maximum::", p)
r=p-q
print("Range::",r)
n=3
a=df['HR'].value_counts(bins=n)
print("\n\n-------------Stress or Non Stress RMSSD Data-------------")
print(a)
df_filtered_1 = df.query('Condition == 1') #stress
df_filtered_0 = df.query('Condition == 0') #non stress

a1 = df_filtered_1['RMSSD'].value_counts(bins=n) #stress
a0 = df_filtered_0['RMSSD'].value_counts(bins=n) #non stress

print("\n------------------Stress Data HRV/RMSSD----------------")
print(a1)
a1.plot()

p=df['SCL'].max()
q=df['SCL'].min()
print("Minimum::", q)
print("Maximum::", p)
r=p-q
print("Range::",r)
n=3
a=df['SCL'].value_counts(bins=n)
print("\n\n-------------Stress or Non Stress SCL Data-------------")
print(a)
df_filtered_1 = df.query('Condition == 1') #stress
df_filtered_0 = df.query('Condition == 0') #non stress

a1 = df_filtered_1['SCL'].value_counts(bins=n) #stress
a0 = df_filtered_0['SCL'].value_counts(bins=n) #non stress

print("\n------------------Stress Data SCL----------------")
print(a1)
a1.plot()

n=12
a=df["Ssad"].value_counts(bins=n)
print("--------------Sad Data-----------")
print(a)
df_filtered_1 = df.query('Condition == 1') #stress
df_filtered_0 = df.query('Condition == 0') #non stress
a1 = df_filtered_1['Ssad'].value_counts(bins=n) #stress
a0 = df_filtered_0['Ssad'].value_counts(bins=n) #non stress
print("\n::::::::::::::::Non stress data:::::::::::::::::")
print(a0)


n=10
a=df["SleftEyeClosed"].value_counts(bins=n)
print("\n--------------Left Eyes Data-----------")
print(a)
df_filtered_1 = df.query('Condition == 1') #stress
df_filtered_0 = df.query('Condition == 0') #non stress
a1 = df_filtered_1['SleftEyeClosed'].value_counts(bins=n) #stress
a0 = df_filtered_0['SleftEyeClosed'].value_counts(bins=n) #non stress
print("\n::::::::::::::::Non stress data:::::::::::::::::")
print(a0)



n=4
a=df["SrightEyeClosed"].value_counts(bins=n)
print("\n--------------Right Eyes Data-----------")
print(a)
df_filtered_1 = df.query('Condition == 1') #stress
df_filtered_0 = df.query('Condition == 0') #non stress
a1 = df_filtered_1['SrightEyeClosed'].value_counts(bins=n) #stress
a0 = df_filtered_0['SrightEyeClosed'].value_counts(bins=n) #non stress
print("\n::::::::::::::::Non stress data:::::::::::::::::")
print(a0)



n=8
a=df["SleftEyebrowLowered"].value_counts(bins=n)
print("\n--------------Left Eyes Data-----------")
print(a)
df_filtered_1 = df.query('Condition == 1') #stress
df_filtered_0 = df.query('Condition == 0') #non stress
a1 = df_filtered_1['SleftEyebrowLowered'].value_counts(bins=n) #stress
a0 = df_filtered_0['SleftEyebrowLowered'].value_counts(bins=n) #non stress
print("\n::::::::::::::::Non stress data:::::::::::::::::")
print(a0)


a=df["SleftEyebrowRaised"].value_counts(bins=n)
print("\n--------------Left Eyes Data-----------")
print(a)
df_filtered_1 = df.query('Condition == 1') #stress
df_filtered_0 = df.query('Condition == 0') #non stress
a1 = df_filtered_1['SleftEyebrowRaised'].value_counts(bins=n) #stress
a0 = df_filtered_0['SleftEyebrowRaised'].value_counts(bins=n) #non stress
print("\n::::::::::::::::Non stress data:::::::::::::::::")
print(a0)
0


a=df["SrightEyebrowLowered"].value_counts(bins=n)
print("\n--------------Left Eyes Data-----------")
print(a)
df_filtered_1 = df.query('Condition == 1') #stress
df_filtered_0 = df.query('Condition == 0') #non stress
a1 = df_filtered_1['SrightEyebrowLowered'].value_counts(bins=n) #stress
a0 = df_filtered_0['SrightEyebrowLowered'].value_counts(bins=n) #non stress
print("\n::::::::::::::::Non stress data:::::::::::::::::")
print(a0)


a=df["SrightEyebrowRaised"].value_counts(bins=n)
print("\n--------------Left Eyes Data-----------")
print(a)
df_filtered_1 = df.query('Condition == 1') #stress
df_filtered_0 = df.query('Condition == 0') #non stress
a1 = df_filtered_1['SrightEyebrowRaised'].value_counts(bins=n) #stress
a0 = df_filtered_0['SrightEyebrowRaised'].value_counts(bins=n) #non stress
print("\n::::::::::::::::Non stress data:::::::::::::::::")
print(a0)



n=15
a=df["Svalence"].value_counts(bins=n)
print("\n--------------Svalence------------")
print(a)
df_filtered_1 = df.query('Condition == 1') #stress
df_filtered_0 = df.query('Condition == 0') #non stress
a1 = df_filtered_1['Svalence'].value_counts(bins=n) #stress
a0 = df_filtered_0['Svalence'].value_counts(bins=n) #non stress
print("\n::::::::::::::::Non stress data:::::::::::::::::")
print(a0)


n=10
a=df["Sangry"].value_counts(bins=n)
print("\n--------------Sangry-------------")
print(a)
df_filtered_1 = df.query('Condition == 1') #stress
df_filtered_0 = df.query('Condition == 0') #non stress
a1 = df_filtered_1['Sangry'].value_counts(bins=n) #stress
a0 = df_filtered_0['Sangry'].value_counts(bins=n) #non stress
print("\n::::::::::::::::Non stress data:::::::::::::::::")
print(a0)


n=15
a=df["Shappy"].value_counts(bins=n)
print("\n------------Shappy-----------")
print(a)
df_filtered_1 = df.query('Condition == 1') #stress
df_filtered_0 = df.query('Condition == 0') #non stress
a1 = df_filtered_1['Shappy'].value_counts(bins=n) #stress
a0 = df_filtered_0['Shappy'].value_counts(bins=n) #non stress
print("\n::::::::::::::::Non stress data:::::::::::::::::")
print(a0)

df.head()

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis
from sklearn.decomposition import PCA

X = df.drop(['Condition'],1)
Y = df['Condition']
#to standardize range
scaler = StandardScaler()
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.20, random_state=1)


X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


print("X_train", len(X_train))
print("X_test", len(X_test))
print("Y_train", len(Y_train))
print("Y_test", len(Y_test))



"""#Part 2"""

#lasso regression
import numpy as np

#load dataset
data = pd.read_csv('/content/drive/My Drive/BDA_Project/BDA_Project/face_behave.csv')
data=data.dropna()
data.shape

x=pd.DataFrame(data.drop(labels=['PP','Condition'], axis=1))
y=pd.DataFrame(data['Condition'])

#Scaling and Splitting dataset
from sklearn.preprocessing import MinMaxScaler
Min_Max = MinMaxScaler()
X = Min_Max.fit_transform(x)
Y = Min_Max.fit_transform(y)
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3, random_state=1)


X_train.shape, X_test.shape

from sklearn.linear_model import Lasso, LogisticRegression
from sklearn.feature_selection import SelectFromModel
sel_ = SelectFromModel(LogisticRegression(C=0.5, penalty='l1', solver='liblinear'))
sel_.fit(X_train, np.ravel(Y_train, order='C'))
sel_.get_support()
X_train = pd.DataFrame(X_train)

selected_feat = X_train.columns[(sel_.get_support())]
print('total features: {}'.format((X_train.shape[1])))
print('selected features: {}'.format(len(selected_feat)))
print('features with coefficients rank to zero: {}'.format(np.sum(sel_.estimator_.coef_ == 0)))

np.sum(sel_.estimator_.coef_ == 0)

removed_feats = X_train.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]
removed_feats

X_train_selected = sel_.transform(X_train)
X_test_selected = sel_.transform(X_test)
X_train_selected.shape, X_test_selected.shape

selected_feat.sort_values()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

clf = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)

clf.fit(X_train_selected, np.ravel(Y_train, order='C'))

y_pred = clf.predict(X_test_selected)

accuracy_score(Y_test, y_pred)



"""# Part 3"""

pip install keras==2.12.0

# Commented out IPython magic to ensure Python compatibility.
import pydot
import pandas as pd
import numpy as np
import seaborn as sns
sns.set()
# %matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
# Ignore earn ings
import warnings
warnings.filterwarnings('ignore')
import sklearn
from sklearn import metrics
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import export_graphviz, DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.exceptions import NotFittedError
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras import optimizers
from keras.wrappers.scikit_learn import KerasClassifier
from IPython.display import display

def display_confusion_matrix(target, prediction, score=None):
  cm = metrics.confusion_matrix(target, prediction)
  plt.figure(figsize=(6,6))
  sns.heatmap(cm, annot=True, fmt=".4f", linewidths=.5, square=True, cmap='BuGn')
  plt.ylabel('Actual label')
  plt.xlabel('Predicted label')
  if score:
    score_title = 'Accuracy Score: {0}'.format(round(score, 5))
    plt.title(score_title, size = 25)


def visualize_tree(tree, feature_names):
  with open("dt.dot", 'w') as f:
    export_graphviz(tree, out_file=f, feature_names=feature_names)
  try:
    subprocess.check_call(["dot", "-Tpng", "dt.dot", "-o", "dt.png"])
  except:
    exit("Could not run dot, ie graphviz, to produce visualization")

def draw_missing_data_table(df):
  total	= df.isnull().sum().sort_values(ascending=False)
  percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)
  missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
  return missing_data

train_df_raw = pd.read_csv('/content/drive/My Drive/BDA_Project/BDA_Project/hr.csv')
train_df_raw.head()

train_df_raw=train_df_raw.dropna()

train_df = train_df_raw.copy()
X = train_df.drop(['Condition'], 1)
Y = train_df[ 'Condition' ]
# be s ca Le our data, i t i s essent i a L fora smooth worh Eng o f the mode Le # Sca L ingmeans that each co Lunns as a 8 mean and a 1 van ance
sc  = StandardScaler()
X=pd.DataFrame(sc.fit_transform(X.values), index=X.index, columns=X.columns)
# SpL itdataset for  /rodeL tes I ing
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)
X_train.head()

rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, Y_train)
rf_prediction = rf.predict(X_test)
score	= metrics.accuracy_score(Y_test, rf_prediction)
display_confusion_matrix(Y_test, rf_prediction, score=score)
rf_score=rf_prediction

dt = DecisionTreeClassifier(min_samples_split=15, min_samples_leaf=20, random_state=0)
dt.fit(X_train, Y_train)
dt_prediction = dt.predict(X_test)

score=metrics.accuracy_score(Y_test, dt_prediction)
display_confusion_matrix(Y_test, dt_prediction, score=score)

svm = SVC(gamma='auto', random_state=42)
svm.fit(X_train, Y_train)
svm_prediction = svm.predict(X_test)

score=metrics.accuracy_score(Y_test, svm_prediction)
display_confusion_matrix(Y_test, svm_prediction, score=score)

#train a Gaus s ian Nai ve Bayes c Las s ifi er on the tr'aining set
from sklearn.naive_bayes import GaussianNB
# ins tant i ate the code L
gnb = GaussianNB()
gnb.fit(X_train, Y_train)
NB_y_pred = gnb.predict(X_test)
from sklearn.metrics import accuracy_score
y_pred_train = gnb.predict(X_train)
y_pred_train
# print the scores on tra in Eng and test set
print('Training set score: {:.4f}'.format(gnb.score(X_train, Y_train)))
print('Test set score: {:.4f}'.format(gnb.score(X_test, Y_test)))
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(Y_test, NB_y_pred)
display_confusion_matrix(Y_test, NB_y_pred, score=score)

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=3)
classifier.fit(X_train, Y_train)
KNN_y_pred = classifier.predict(X_test)
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(Y_test, KNN_y_pred))
print(classification_report(Y_test, KNN_y_pred))

print ("********************* HR *********************")
print('SVM ----›',metrics.accuracy_score(Y_test, svm_prediction))
print('RF ---->',metrics.accuracy_score(Y_test, rf_prediction))
print('DT ----›',metrics.accuracy_score(Y_test, dt_prediction))
print('NB ----›',metrics.accuracy_score(Y_test, NB_y_pred))
print('KNN ----›',metrics.accuracy_score(Y_test, KNN_y_pred))



"""Confusion Matrix
As we can see from the table above:

True Positive(TP) : Values that the model predicted as yes, and is actually yes. True Negative(TN) : Values that the model predicted as not, and is actually no. False Positive(FP): Values that the model predicted as yes, but actually no.
False Negative(FN): Values that the model predicted as no, but actually yes.
"""

from sklearn.metrics import precision_score, recall_score, confusion_matrix
cm	= np.array(confusion_matrix(Y_test, rf_prediction, labels=[0,1]))
confusion_mat= pd.DataFrame(cm, index = ['NORMAL', 'STRESSED'],
columns =['NORMAL','STRESSED'])
confusion_mat

sns.heatmap(cm,annot=True,fmt='g',cmap='Set3')



"""Accuracy_Score

Accuracy_Score is the most intuitive performance measure and it is simply a ratio of correctly
predicted observation to the total observations
"""

from sklearn.metrics import accuracy_score
print(accuracy_score(Y_test, rf_prediction))



"""Precision

Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.
"""

print(precision_score(Y_test, rf_prediction))

"""Recall
Recall also called Sensitivity, is the ratio of positive instances that are correctly detected by the
classifier to the all observations in actual class

"""

print(recall_score(Y_test, rf_prediction))

"""Classification Report"""

from sklearn.metrics import classification_report
print(classification_report(Y_test, rf_prediction))



"""The ROC Curve

Area Under Curve

Area Under Curve is a common way to compare classifiers. A perfect classifier will have ROC AUC equal to 1
Sckit-Learn provides a function to compute the ROC AUC.
"""

from sklearn.metrics import roc_auc_score,auc,f1_score
from sklearn.metrics import RocCurveDisplay
ax = plt.gca()
rfc_disp = RocCurveDisplay(rf, X_test, Y_test, ax=ax)
plt.show()



"""K FOLD VALIDATION"""

cv_score_rf1 = cross_val_score(estimator=rf, X=X_train, y=Y_train, cv=5, n_jobs=3)
cv_score_rf2 = cross_val_score(estimator=rf, X=X_train, y=Y_train, cv=10, n_jobs=3)
cv_score_rf3 = cross_val_score(estimator=rf, X=X_train, y=Y_train, cv=20, n_jobs=2)
cv_score_rf4 = cross_val_score(estimator=rf, X=X_train, y=Y_train, cv=50, n_jobs=3)

cv_result = { 'rf5': cv_score_rf1, 'rf10': cv_score_rf2, 'rf20': cv_score_rf3, 'rf40': cv_score_rf4}
cv_data = {model: [score.mean(), score.std()] for model, score in cv_result.items}
cv_df=pd.DataFrame(cv_data, index=['Mean_accuracy', 'Variance'])
cv_df



"""HR And Gsr

# Part 4
"""

# Commented out IPython magic to ensure Python compatibility.
import pydot
import pandas as pd
import numpy as np
import seaborn as sns
sns.set()
# %matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
# Ignore warnings
import warnings
warnings.filterwarnings('ignore')
import sklearn
from sklearn import metrics
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import export_graphviz, DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.exceptions import NotFittedError
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras import optimizers
from keras.wrappers.scikit_learn import KerasClassifier
from IPython.display import display

# Some usefuL functions we'LL use in this notebook
def display_confusion_matrix(target, prediction, score=None):
  cm = metrics.confusion_matrix(target, prediction)
  plt.figure(figsize=(6,6))
  sns.heatmap(cm, annot=True, fmt=".4f", linewidths=.5, square=True, cmap='BuGn')
  plt.ylabel('Actual label')
  plt.xlabel('Predicted label')
  if score:
    score_title = 'Accuracy Score: {0}'.format(round(score, 5))
    plt.title(score_title, size = 25)
def visualize_tree(tree, feature_names):
  with open("dt.dot", 'w') as f:
    export_graphviz(tree, out_file=f, feature_names=feature_names)
  try:
    subprocess.check_call(["dot", "-Tpng", "dt.dot", "-o", "dt.png"])
  except:
    exit("Could not run dot, ie graphviz, to produce visualization")
def draw_missing_data_table(df):
  total = df.isnull().sum().sort_values(ascending=False)
  percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)
  missing_data = pd.concat([total, percent], axis , keys=['Total', 'Percent'])
  return missing_data

# Create dataframe for training dataset and print five first rows as preview
train_df_raw = pd.read_csv('/content/drive/My Drive/BDA_Project/BDA_Project/hr_gsr.csv')
train_df_raw.head()

train_df_raw = train_df_raw.dropna()

# Let's divide the train dataset in two datasets to evaLuate perfomance of the
train_df = train_df_raw.copy()
X = train_df.drop(['Condition'], 1)
Y = train_df['Condition']
    # • We scaLe our data, it is essentiaL for a smooth working of the modeLs
    # • ScaLing means that each coLumns as a 0 mean and a 1 variance
sc = StandardScaler()
X = pd.DataFrame(sc.fit_transform(X.values), index=X.index, columns=X.columns)
  #  • SpLit dataset for modeL testing
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)
X_train.head()

rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, Y_train)
rf_prediction = rf.predict(X_test)
score	= metrics.accuracy_score(Y_test, rf_prediction)
display_confusion_matrix(Y_test, rf_prediction, score=score)
rf_score=rf_prediction

dt = DecisionTreeClassifier(min_samples_split=15, min_samples_leaf=20, random_state=0)
dt.fit(X_train, Y_train)
dt_prediction = dt.predict(X_test)

score=metrics.accuracy_score(Y_test, dt_prediction)
display_confusion_matrix(Y_test, dt_prediction, score=score)

svm = SVC(gamma='auto', random_state=42)
svm.fit(X_train, Y_train)
svm_prediction = svm.predict(X_test)

score=metrics.accuracy_score(Y_test, svm_prediction)
display_confusion_matrix(Y_test, svm_prediction, score=score)

#train a Gaus s ian Nai ve Bayes c Las s ifi er on the tr'aining set
from sklearn.naive_bayes import GaussianNB
# ins tant i ate the code L
gnb = GaussianNB()
gnb.fit(X_train, Y_train)
NB_y_pred = gnb.predict(X_test)
from sklearn.metrics import accuracy_score
y_pred_train = gnb.predict(X_train)
y_pred_train
# print the scores on tra in Eng and test set
print('Training set score: {:.4f}'.format(gnb.score(X_train, Y_train)))
print('Test set score: {:.4f}'.format(gnb.score(X_test, Y_test)))
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(Y_test, NB_y_pred)
display_confusion_matrix(Y_test, NB_y_pred, score=score)

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=3)
classifier.fit(X_train, Y_train)
KNN_y_pred = classifier.predict(X_test)
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(Y_test, KNN_y_pred))
print(classification_report(Y_test, KNN_y_pred))

print ("********************* HR GSR *********************")
print('SVM ----›',metrics.accuracy_score(Y_test, svm_prediction))
print('RF ---->',metrics.accuracy_score(Y_test, rf_prediction))
print('DT ----›',metrics.accuracy_score(Y_test, dt_prediction))
print('NB ----›',metrics.accuracy_score(Y_test, NB_y_pred))
print('KNN ----›',metrics.accuracy_score(Y_test, KNN_y_pred))

from sklearn.metrics import precision_score, recall_score, confusion_matrix
cm	= np.array(confusion_matrix(Y_test, rf_prediction, labels=[0,1]))
confusion_mat= pd.DataFrame(cm, index = ['NORMAL', 'STRESSED'],
columns =['NORMAL','STRESSED'])
confusion_mat

sns.heatmap(cm,annot=True,fmt='g',cmap='Set3')

from sklearn.metrics import accuracy_score
print(accuracy_score(Y_test, rf_prediction))

print(precision_score(Y_test, rf_prediction))

print(recall_score(Y_test, rf_prediction))

from sklearn.metrics import classification_report
print(classification_report(Y_test, rf_prediction))

from sklearn.metrics import roc_auc_score,auc,f1_score
from sklearn.metrics import RocCurveDisplay
ax = plt.gca()
rfc_disp = RocCurveDisplay(rf, X_test, Y_test, ax=ax)
plt.show()

cv_score_rf1 = cross_val_score(estimator=rf, X=X_train, y=Y_train, cv=5, n_jobs=3)
cv_score_rf2 = cross_val_score(estimator=rf, X=X_train, y=Y_train, cv=10, n_jobs=3)
cv_score_rf3 = cross_val_score(estimator=rf, X=X_train, y=Y_train, cv=20, n_jobs=2)
cv_score_rf4 = cross_val_score(estimator=rf, X=X_train, y=Y_train, cv=50, n_jobs=3)

cv_result = { 'rf5': cv_score_rf1, 'rf10': cv_score_rf2, 'rf20': cv_score_rf3, 'rf40': cv_score_rf4}
cv_data = {model: [score.mean(), score.std()] for model, score in cv_result.items}
cv_df=pd.DataFrame(cv_data, index=['Mean_accuracy', 'Variance'])
cv_df

train_df_raw = pd.read_csv('/content/drive/My Drive/BDA_Project/BDA_Project/face_hr.csv')
train_df_raw.head()

train_df_raw=train_df_raw.dropna()

train_df = train_df_raw.copy()
X = train_df.drop(['Condition'], 1)
Y = train_df[ 'Condition' ]
# be s ca Le our data, i t i s essent i a L fora smooth worh Eng o f the mode Le # Sca L ingmeans that each co Lunns as a 8 mean and a 1 van ance
sc  = StandardScaler()
X=pd.DataFrame(sc.fit_transform(X.values), index=X.index, columns=X.columns)
# SpL itdataset for  /rodeL tes I ing
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)
X_train.head()

rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, Y_train)
rf_prediction = rf.predict(X_test)
score	= metrics.accuracy_score(Y_test, rf_prediction)
display_confusion_matrix(Y_test, rf_prediction, score=score)
rf_score=rf_prediction

dt = DecisionTreeClassifier(min_samples_split=15, min_samples_leaf=20, random_state=0)
dt.fit(X_train, Y_train)
dt_prediction = dt.predict(X_test)

score=metrics.accuracy_score(Y_test, dt_prediction)
display_confusion_matrix(Y_test, dt_prediction, score=score)

svm = SVC(gamma='auto', random_state=42)
svm.fit(X_train, Y_train)
svm_prediction = svm.predict(X_test)

score=metrics.accuracy_score(Y_test, svm_prediction)
display_confusion_matrix(Y_test, svm_prediction, score=score)

#train a Gaus s ian Nai ve Bayes c Las s ifi er on the tr'aining set
from sklearn.naive_bayes import GaussianNB
# ins tant i ate the code L
gnb = GaussianNB()
gnb.fit(X_train, Y_train)
NB_y_pred = gnb.predict(X_test)
from sklearn.metrics import accuracy_score
y_pred_train = gnb.predict(X_train)
y_pred_train
# print the scores on tra in Eng and test set
print('Training set score: {:.4f}'.format(gnb.score(X_train, Y_train)))
print('Test set score: {:.4f}'.format(gnb.score(X_test, Y_test)))
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(Y_test, NB_y_pred)
display_confusion_matrix(Y_test, NB_y_pred, score=score)

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=3)
classifier.fit(X_train, Y_train)
KNN_y_pred = classifier.predict(X_test)
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(Y_test, KNN_y_pred))
print(classification_report(Y_test, KNN_y_pred))

print ("********************* HR And Face *********************")
print('SVM ----›',metrics.accuracy_score(Y_test, svm_prediction))
print('RF ---->',metrics.accuracy_score(Y_test, rf_prediction))
print('DT ----›',metrics.accuracy_score(Y_test, dt_prediction))
print('NB ----›',metrics.accuracy_score(Y_test, NB_y_pred))
print('KNN ----›',metrics.accuracy_score(Y_test, KNN_y_pred))

from sklearn.metrics import precision_score, recall_score, confusion_matrix
cm	= np.array(confusion_matrix(Y_test, rf_prediction, labels=[0,1]))
confusion_mat= pd.DataFrame(cm, index = ['NORMAL', 'STRESSED'],
columns =['NORMAL','STRESSED'])
confusion_mat

sns.heatmap(cm,annot=True,fmt='g',cmap='Set3')

from sklearn.metrics import accuracy_score
print(accuracy_score(Y_test, rf_prediction))

print(precision_score(Y_test, rf_prediction))

print(recall_score(Y_test, rf_prediction))

from sklearn.metrics import classification_report
print(classification_report(Y_test, rf_prediction))

from sklearn.metrics import roc_auc_score,auc,f1_score
from sklearn.metrics import RocCurveDisplay
ax = plt.gca()
rfc_disp = RocCurveDisplay(rf, X_test, Y_test, ax=ax)
plt.show()

cv_score_rf1 = cross_val_score(estimator=rf, X=X_train, y=Y_train, cv=5, n_jobs=3)
cv_score_rf2 = cross_val_score(estimator=rf, X=X_train, y=Y_train, cv=10, n_jobs=3)
cv_score_rf3 = cross_val_score(estimator=rf, X=X_train, y=Y_train, cv=20, n_jobs=2)
cv_score_rf4 = cross_val_score(estimator=rf, X=X_train, y=Y_train, cv=50, n_jobs=3)

cv_result = { 'rf5': cv_score_rf1, 'rf10': cv_score_rf2, 'rf20': cv_score_rf3, 'rf40': cv_score_rf4}
cv_data = {model: [score.mean(), score.std()] for model, score in cv_result.items}
cv_df=pd.DataFrame(cv_data, index=['Mean_accuracy', 'Variance'])
cv_df



"""HR GSR and Facial image data"""

train_df_raw = pd.read_csv('/content/drive/My Drive/BDA_Project/BDA_Project/face_phy.csv')
train_df_raw.head()

train_df_raw=train_df_raw.dropna()

train_df = train_df_raw.copy()
X = train_df.drop(['Condition'], 1)
Y = train_df[ 'Condition' ]
# be s ca Le our data, i t i s essent i a L fora smooth worh Eng o f the mode Le # Sca L ingmeans that each co Lunns as a 8 mean and a 1 van ance
sc  = StandardScaler()
X=pd.DataFrame(sc.fit_transform(X.values), index=X.index, columns=X.columns)
# SpL itdataset for  /rodeL tes I ing
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)
X_train.head()

rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, Y_train)
rf_prediction = rf.predict(X_test)
score	= metrics.accuracy_score(Y_test, rf_prediction)
display_confusion_matrix(Y_test, rf_prediction, score=score)
rf_score=rf_prediction

dt = DecisionTreeClassifier(min_samples_split=15, min_samples_leaf=20, random_state=0)
dt.fit(X_train, Y_train)
dt_prediction = dt.predict(X_test)

score=metrics.accuracy_score(Y_test, dt_prediction)
display_confusion_matrix(Y_test, dt_prediction, score=score)

dt = DecisionTreeClassifier(min_samples_split=15, min_samples_leaf=20, random_state=0)
dt.fit(X_train, Y_train)
dt_prediction = dt.predict(X_test)

score=metrics.accuracy_score(Y_test, dt_prediction)
display_confusion_matrix(Y_test, dt_prediction, score=score)

svm = SVC(gamma='auto', random_state=42)
svm.fit(X_train, Y_train)
svm_prediction = svm.predict(X_test)

score=metrics.accuracy_score(Y_test, svm_prediction)
display_confusion_matrix(Y_test, svm_prediction, score=score)

#train a Gaus s ian Nai ve Bayes c Las s ifi er on the tr'aining set
from sklearn.naive_bayes import GaussianNB
# ins tant i ate the code L
gnb = GaussianNB()
gnb.fit(X_train, Y_train)
NB_y_pred = gnb.predict(X_test)
from sklearn.metrics import accuracy_score
y_pred_train = gnb.predict(X_train)
y_pred_train
# print the scores on tra in Eng and test set
print('Training set score: {:.4f}'.format(gnb.score(X_train, Y_train)))
print('Test set score: {:.4f}'.format(gnb.score(X_test, Y_test)))
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(Y_test, NB_y_pred)
display_confusion_matrix(Y_test, NB_y_pred, score=score)

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=3)
classifier.fit(X_train, Y_train)
KNN_y_pred = classifier.predict(X_test)
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(Y_test, KNN_y_pred))
print(classification_report(Y_test, KNN_y_pred))

print ("********************* HR and facial data*********************")
print('SVM ----›',metrics.accuracy_score(Y_test, svm_prediction))
print('RF ---->',metrics.accuracy_score(Y_test, rf_prediction))
print('DT ----›',metrics.accuracy_score(Y_test, dt_prediction))
print('NB ----›',metrics.accuracy_score(Y_test, NB_y_pred))
print('KNN ----›',metrics.accuracy_score(Y_test, KNN_y_pred))

from sklearn.metrics import precision_score, recall_score, confusion_matrix
cm	= np.array(confusion_matrix(Y_test, rf_prediction, labels=[0,1]))
confusion_mat= pd.DataFrame(cm, index = ['NORMAL', 'STRESSED'],
columns =['NORMAL','STRESSED'])
confusion_mat

sns.heatmap(cm,annot=True,fmt='g',cmap='Set3')

from sklearn.metrics import accuracy_score
print(accuracy_score(Y_test, rf_prediction))

print(precision_score(Y_test, rf_prediction))

print(recall_score(Y_test, rf_prediction))

from sklearn.metrics import classification_report
print(classification_report(Y_test, rf_prediction))

from sklearn.metrics import roc_auc_score,auc,f1_score
from sklearn.metrics import RocCurveDisplay
ax = plt.gca()
rfc_disp = RocCurveDisplay(rf, X_test, Y_test, ax=ax)
plt.show()

cv_score_rf1 = cross_val_score(estimator=rf, X=X_train, y=Y_train, cv=5, n_jobs=3)
cv_score_rf2 = cross_val_score(estimator=rf, X=X_train, y=Y_train, cv=10, n_jobs=3)
cv_score_rf3 = cross_val_score(estimator=rf, X=X_train, y=Y_train, cv=20, n_jobs=2)
cv_score_rf4 = cross_val_score(estimator=rf, X=X_train, y=Y_train, cv=50, n_jobs=3)

cv_result = { 'rf5': cv_score_rf1, 'rf10': cv_score_rf2, 'rf20': cv_score_rf3, 'rf40': cv_score_rf4}
cv_data = {model: [score.mean(), score.std()] for model, score in cv_result.items}
cv_df=pd.DataFrame(cv_data, index=['Mean_accuracy', 'Variance'])
cv_df

dictionary = {"model": ['KNN', 'SVM', 'DT', 'RF'], "score": [metrics.accuracy_score(Y_test, KNN_y_pred), metrics.accuracy_score(Y_test, svm_prediction),metrics.accuracy_score(Y_test, dt_prediction), metrics.accuracy_score(Y_test, rf_prediction)]}
df1 = pd.DataFrame(dictionary)
df1

df1.head()

pip install plotly

import plotly.graph_objs as go
import plotly
plotly.offline.init_notebook_mode()

new_index5 = df1.score.sort_values(ascending=False).index.values
sorted_data5=df1.reindex(new_index5)

# creating trace

trace1 = go.Bar(
    x = sorted_data5.model,
    y = sorted_data5.score,
    name = "score",
    marker = dict(color = 'rgba(100,10,10,0.5)',
                  line = dict(color = 'rgb(10,10,0)')),
    text = sorted_data5.model)
dat = [trace1]
layout = go.Layout(barmode="group", title = 'scores of classification')
fig = go.Figure(data=dat, layout=layout)
plotly.offline.iplot(fig)

X=['RF']
Yprecision=[88.81]
Zrecall=[91.70]

X_axis=np.arange(len(X))

plt.bar(X_axis-0.2, Yprecision, 0.4, lael='precision', color='#7f6f5f')
plt.bar(X_axis-0.4, Zrecall, 0.4, lael='recall', color='#2d7f5e')
plt.grid(color='#95a5a9', linestyle='--', linewidth=2, alpha=0.2)

plt.xticks(X_axis, X)
plt.xlabel("CLASSIFICATION MODEL EVALUATION")
plt.ylabel("ACCURACY IN PERCENTAGE(% )")
plt.title("CLASSIFICATION ANALYSIS")
plt.legend()
plt.show()

plotdata = pd.DataFrame({
    "Accuracy": [88.48],
    "precision": [88.81],
    "recall": [91.70],
    "f-measure":[90.23]
  },
  index=["RF"]
)

ax = pl0tdata.plot(kind="bar", width=0.6)
plt.grid(color='#95a5a9', linestyle='--', linewidth=2, alpha=0.2)

for p in ax.pathces:
  ax.annotate(str(p.get_hieght()), (p.get_x()*1.005, p.get_height()*1.005))
plt.xlabel("CLASSIFICATION MODEL EVALUATION")
plt.ylabel("ACCURACY IN PERCENTAGE(% )")
plt.title("CLASSIFICATION ANALYSIS")

X = ['RANDOM FOREST']
correct = [423]
miss = [55]

X_axis = np.arange(len(X))

plt.bar(X_axis-0.2, correct, 0.3, label='CORRECTLY CLASSIFY', color='#7f6f5f')
plt.bar(X_axis-0.6, miss, 0.3, lael='MISSCLASSIFY', color='#2d7f5e')
plt.grid(color='#95a5a9', linestyle='--', linewidth=2, alpha=0.2)

plt.xticks(X_axis, X)
plt.xlabel("CLASSIFICATION MODEL EVALUATION")
plt.ylabel("ACCURACY IN PERCENTAGE(% )")
plt.title("CLASSIFICATION ANALYSIS")
plt.legend()
plt.show()

plotdata = pd.DataFrame({
    "SVM": [60,58.15,73.26],
    "DT": [67.37,66.22,65.34],
    "RF": [77.86,68.20,77.66],
    "KNN":[74.36,67.22,73.27]
  },
  index=["HRV","GSR","FACIAL"]
)

ax = plotdata.plot(kind="bar", width=0.6)
plt.grid(color='#95a5a9', linestyle='--', linewidth=2, alpha=0.2)

for p in ax.pathces:
  ax.annotate(str(p.get_hieght()), (p.get_x()*1.005, p.get_height()*1.005))
plt.xlabel("CLASSIFICATION MODEL EVALUATION")
plt.ylabel("ACCURACY IN PERCENTAGE(% )")
plt.title("CLASSIFICATION ANALYSIS")

plotdata = pd.DataFrame({
    "RF": [87.22,87.43,87.95,87.96]
  },
  index=['k=5','k=10','k=20','k=50']
)

ax = plotdata.plot(kind="line", width=0.6)
plt.grid(color='#95a5a9', linestyle='--', linewidth=2, alpha=0.2)

for p in ax.pathces:
  ax.annotate(str(p.get_hieght()), (p.get_x()*1.005, p.get_height()*1.005))
plt.xlabel("CROSS VALIDATION MODEL EVALUATION")
plt.ylabel("ACCURACY IN PERCENTAGE(% )")
plt.title("CROSS VALIDATION WITH K VALUE")

X=['HR', 'GSR','FACIAL', 'HR+GSR','GSR+FACIAL', 'HR+GSR+FACIAL']
SVM=[60,58.15,73.26,59.53,76.18,78.97,83.26]
DT=[67.37,66.22,65.14,73.67,72.15,68.45,71.54]
RF=[77.86,68.20,77.66,82.21,84.59,85.77,88.49]
KNN=[74.36,67.22,73.27,76.42,77.23,81.86,82]

X_axis=np.arange(len(X))

plt.bar(X_axis-0.2, SVM, 0.4, lael='SVM', color='green',edgecolor='black')
plt.bar(X_axis+0.2, DT, 0.4, lael='DT', color='red',edgecolor='black')
plt.bar(X_axis+0.2, RF, 0.4, lael='RF', color='green',edgecolor='black')
plt.bar(X_axis+0.2, KNN, 0.4, lael='KNN', color='red',edgecolor='black')

plt.xticks(X_axis, X)
plt.xlabel("CLASSIFICATION MODEL EVALUATION")
plt.ylabel("ACCURACY IN PERCENTAGE(% )")
plt.title("CLASSIFICATION ANALYSIS")
plt.legend()
plt.show()

X = ['HR+GSR','GSR+FACIAL', 'HR+GSR+FACIAL']

RF = [82.21,84.59,85.77,88.49]

X_axis=np.arange(len(X))

plt.var(X_axis-0.2,RF, 0.5, color='#7f6f5f',edgecolr='black')

plt.xticks(X_axis, X)
plt.xlabel("CLASSIFICATION MODEL EVALUATION")
plt.ylabel("ACCURACY IN PERCENTAGE(% )")
plt.title("CLASSIFICATION ANALYSIS")
plt.legend()
plt.show()

from skelarn import metrics

print('Mean Absolute Error', metrics.mean_absolute_error(Y_test, rf_prediction))
print('Mean Squared Error', metrics.mean_squared_error(Y_test, rf_prediction))
print('Root Mean Squared Error', nq.sqrt(metrics.mean_absolute_error(Y_test, rf_prediction)))

print('----------------Confusion Matrix------------------')
print(confusion_matrix(Y_test, rf_prediction))
print('\n')
print('----------------Classification Report---------------')
print(classification_report(Y_test, rf_prediction))